{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ade75c-3ec4-46cf-8889-05fadaa72d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.conda/envs/cos/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-24 18:06:27 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 18:06:31,256\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import io\n",
    "\n",
    "import transformers  \n",
    "from transformers import PretrainedConfig, PreTrainedModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, AutoModel\n",
    "\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "from vllm.entrypoints.llm import LLM\n",
    "from vllm import ModelRegistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87e7f7a-842c-480f-9823-832b09fb884d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa123895-01ae-4290-897e-5b64a5601472",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_name = \"/home/sagemaker-user/efs/model/Qwen3-0.6B\"\n",
    "target_name = \"/home/sagemaker-user/efs/model/Qwen3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adadec8a-2941-46ab-afec-c232442688b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleConfig(PretrainedConfig):\n",
    "    model_type = \"customize_ensemble\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=4096,\n",
    "        vocab_size=151936,\n",
    "        target_model_path=None,\n",
    "        draft_model_path=None,\n",
    "        trust_remote_code=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ensemble_hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.target_model_path = target_model_path\n",
    "        self.draft_model_path = draft_model_path\n",
    "        self.trust_remote_code=trust_remote_code\n",
    "\n",
    "class EnsembleModel(PreTrainedModel):\n",
    "    config_class = EnsembleConfig\n",
    "\n",
    "    def __init__(self, config: EnsembleConfig):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.target_model = AutoModel.from_pretrained(config.target_model_path)\n",
    "        self.draft_model = AutoModel.from_pretrained(config.draft_model_path)\n",
    "        \n",
    "        self.ensemble_head = nn.Linear(config.ensemble_hidden_size, 2, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        draft_logits = self.draft_model(input_ids, **kwargs).logits  # [B, T, V]\n",
    "        target_output = self.target_model(input_ids, output_hidden_states=True, **kwargs)\n",
    "        target_logits = target_output.logits                          # [B, T, V]\n",
    "        last_hidden = target_output.hidden_states[-1]                # [B, T, H]\n",
    "\n",
    "        # Compute ensemble weights\n",
    "        weights = F.softmax(self.ensemble_head(last_hidden), dim=-1)  # [B, T, 2]\n",
    "\n",
    "        # Expand weights to match logits shape\n",
    "        w_draft = weights[..., 0].unsqueeze(-1)  # [B, T, 1]\n",
    "        w_target = weights[..., 1].unsqueeze(-1)\n",
    "\n",
    "        # Weighted logits\n",
    "        ensemble_logits = w_draft * draft_logits + w_target * target_logits\n",
    "\n",
    "        return CausalLMOutput(logits=ensemble_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "261c3f3b-7f2c-419a-b461-4b1cdba2f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_model(target_name, draft_name):\n",
    "\n",
    "    target_config = AutoConfig.from_pretrained(target_name)\n",
    "    draft_config = AutoConfig.from_pretrained(draft_name)\n",
    "    hidden_size = target_config.vocab_size+draft_config.vocab_size+target_config.hidden_size\n",
    "    config = EnsembleConfig(hidden_size=hidden_size, vocab_size=target_config.vocab_size, \n",
    "                            target_model_path=target_name, draft_model_path=draft_name)\n",
    "    \n",
    "    return EnsembleModel(config), config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01896a6a-d974-4bb3-93d2-5410939a9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoConfig.register(\"customize_ensemble\", EnsembleConfig)\n",
    "AutoModel.register(EnsembleConfig, EnsembleModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eecda376-1d95-4a7b-a587-bbfe4cce8764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "model, config = create_ensemble_model(target_name, draft_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75796773-fb30-4fd1-93f4-a7cb089c60e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.auto_map = {\n",
    "    \"AutoConfig\": \"modeling.configuration:EnsembleConfig\",\n",
    "    \"AutoModel\": \"modeling.ensemble_model:EnsembleModel\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9248924e-410c-4758-8df8-ce947d7cb1c9",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "\n",
    "config.save_pretrained(\"/home/sagemaker-user/efs/ensemble_model/Qwen3-8B_Qwen3-0.6B\")\n",
    "tokenizer.save_pretrained(\"/home/sagemaker-user/efs/ensemble_model/Qwen3-8B_Qwen3-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe196e-cb4c-4c7a-a742-bc22e10a6cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/home/sagemaker-user/efs/ensemble_model/Qwen3-8B_Qwen3-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "154b69c5-571a-40f3-a307-5ceea91ffc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_config = AutoConfig.from_pretrained(\"/home/sagemaker-user/efs/ensemble_model/Qwen3-8B_Qwen3-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1c7d0c5-574b-4b3d-9fda-40f5a638bee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnsembleConfig {\n",
      "  \"architectures\": [\n",
      "    \"EnsembleModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"modeling.configuration:EnsembleConfig\",\n",
      "    \"AutoModel\": \"modeling.ensemble_model:EnsembleModel\"\n",
      "  },\n",
      "  \"draft_model_path\": \"/home/sagemaker-user/efs/model/Qwen3-0.6B\",\n",
      "  \"hidden_size\": 307968,\n",
      "  \"model_type\": \"customize_ensemble\",\n",
      "  \"target_model_path\": \"/home/sagemaker-user/efs/model/Qwen3-8B\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"trust_remote_code\": true,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ensemble_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "256d7eb2-31d0-481a-9a01-ed79dd777188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 57.88it/s]\n"
     ]
    }
   ],
   "source": [
    "ensemble_model = AutoModel.from_pretrained(\"/home/sagemaker-user/efs/ensemble_model/Qwen3-8B_Qwen3-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "708782d7-1b10-447c-8c46-00a223241194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnsembleModel(\n",
      "  (target_model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-35): 36 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
      "          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (draft_model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (ensemble_head): Linear(in_features=307968, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(ensemble_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f74815-f6d3-4e28-9185-b2b6d6ddf301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151936\n",
      "2560\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da2fa419-f4b9-4b49-9463-7aa6647ede46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: could not create work tree dir 'vllm': No space left on device\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0540fe-ca5a-4c89-9435-c5c895f6855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "export VLLM_PRECOMPILED_WHEEL_LOCATION=https://github.com/vllm-project/vllm/releases/download/v0.9.1/vllm-0.9.1+cu126-cp38-abi3-manylinux1_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84729f56-7a4f-4f57-87a7-abf84f10cc46",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ensemble_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mensemble_model\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EnsembleConfig\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ensemble_model'"
     ]
    }
   ],
   "source": [
    "from ensemble_model.configuration import EnsembleConfig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cos",
   "language": "python",
   "name": "cos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
